{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Multiple views of a storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "[torch.FloatTensor of size 13x13]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "x = Tensor(13,13).fill_(1.0)\n",
    "print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    1     2     1     1     1     1     2     1     1     1     1     2     1\n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "    1     2     1     1     1     1     2     1     1     1     1     2     1\n",
      "    1     2     1     1     1     1     2     1     1     1     1     2     1\n",
      "    1     2     1     1     1     1     2     1     1     1     1     2     1\n",
      "    1     2     1     1     1     1     2     1     1     1     1     2     1\n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "    1     2     1     1     1     1     2     1     1     1     1     2     1\n",
      "    1     2     1     1     1     1     2     1     1     1     1     2     1\n",
      "    1     2     1     1     1     1     2     1     1     1     1     2     1\n",
      "    1     2     1     1     1     1     2     1     1     1     1     2     1\n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "    1     2     1     1     1     1     2     1     1     1     1     2     1\n",
      "[torch.FloatTensor of size 13x13]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fill lines with 2\n",
    "x.narrow(0,1,1).fill_(2.0)\n",
    "x.narrow(0,6,1).fill_(2.0)\n",
    "x.narrow(0,11,1).fill_(2.0)\n",
    "\n",
    "#Fill columns with 2\n",
    "x.narrow(1,1,1).fill_(2.0)\n",
    "x.narrow(1,6,1).fill_(2.0)\n",
    "x.narrow(1,11,1).fill_(2.0)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    1     2     1     1     1     1     2     1     1     1     1     2     1\n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "    1     2     1     1     1     1     2     1     1     1     1     2     1\n",
      "    1     2     1     3     3     1     2     1     3     3     1     2     1\n",
      "    1     2     1     3     3     1     2     1     3     3     1     2     1\n",
      "    1     2     1     1     1     1     2     1     1     1     1     2     1\n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "    1     2     1     1     1     1     2     1     1     1     1     2     1\n",
      "    1     2     1     3     3     1     2     1     3     3     1     2     1\n",
      "    1     2     1     3     3     1     2     1     3     3     1     2     1\n",
      "    1     2     1     1     1     1     2     1     1     1     1     2     1\n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "    1     2     1     1     1     1     2     1     1     1     1     2     1\n",
      "[torch.FloatTensor of size 13x13]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.narrow(0, 3, 2).narrow(1, 3, 2).fill_(3)\n",
    "x.narrow(0, 3, 2).narrow(1, 8, 2).fill_(3)\n",
    "x.narrow(0, 8, 2).narrow(1, 3, 2).fill_(3)\n",
    "x.narrow(0, 8, 2).narrow(1, 8, 2).fill_(3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Eigendecomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 7 \n",
       " -6.5567   9.2590   0.6151   0.8370  -7.6169  11.8595  -7.5993  17.4707\n",
       " -0.4058  14.0173  -1.2877  -1.5082 -14.6846   4.5543   0.0217   6.6582\n",
       " 11.4894  -3.1725   7.0358  -0.2557   1.4551  -7.9296   3.3030 -14.0970\n",
       "-11.0053   4.2938  -4.1770   8.2457 -11.5383   8.0959  -9.0505  15.4344\n",
       "  0.4139   2.8804  -4.0275  -1.5159  13.0997   1.4908   0.3803  -3.4121\n",
       " 13.4118  -0.4523 -12.3578  -3.1238   1.5148   2.2164   3.1994 -19.4701\n",
       " -4.3769   5.5507  -3.8987  -2.3709  -2.1417   4.9721   8.0781   2.9932\n",
       "-17.3076   4.8339   7.4778  -0.2427  -1.0392  10.6707  -8.4323  32.9153\n",
       " -1.8437  -0.4491  -5.8529  -4.8591 -13.2296   3.1150   0.8974   6.7957\n",
       " -6.7394  -1.1416   6.0798   2.2818  -5.7883   3.4813   0.8164  10.2430\n",
       " -4.1733  -2.8118   2.5149   1.9973  -5.7264  -1.3467  -2.8704   5.0387\n",
       "  0.3336  -2.6764   3.4557   0.8824  -1.9500  -1.5022   0.6715   1.9423\n",
       " 10.9265  -1.0632  -7.5552  -0.5541  13.1380  -6.3806   1.3289 -16.6006\n",
       " 14.3780  -4.1841  -4.8944   0.9125   4.4502 -10.5475   5.9750 -19.0299\n",
       " -5.3072  -0.4473   3.8308  -0.3494   2.4657   2.9686  -2.4401   8.4184\n",
       "  1.2099  -4.7008   2.1975  -2.3569  -1.5543  -4.9135  -0.7188   5.9657\n",
       "  9.0370   2.5261  -5.4048   1.4125   4.2528  -3.1892   3.9663 -17.2233\n",
       " -9.4150   3.0100   2.3960   0.5775   2.3026   6.4317  -6.0983   8.9975\n",
       " -8.8707   4.2273  -0.8053   2.6576   0.5408   8.0360  -7.2993   8.3222\n",
       " -5.8616  -0.3840   5.6089  -0.5830  -1.0982   1.6181  -0.8592  10.1182\n",
       "\n",
       "Columns 8 to 15 \n",
       " -0.2768 -12.4055   5.4547  19.2968  -4.6445  -6.9424   7.0993 -14.3979\n",
       " -4.8718  -5.1171   1.6620  -2.3843  -5.7988   0.4966  -1.8468  -6.1807\n",
       " -2.4879   5.3987  -4.8258 -12.3203   0.4150   7.2132  -1.1537   6.8094\n",
       "  0.6788  -8.9113  10.1135  17.7028  -3.6257  -7.4497   0.5288 -11.5520\n",
       " -2.7908  -0.4725  -1.0321   1.9792   3.7047  -0.8823   0.5687   1.7600\n",
       " -1.6793   7.1133  -6.3900  -6.6675   9.7629   6.3420   1.4870   3.5048\n",
       " -0.1623  -3.7823   1.7364   9.1157   1.3238  -3.0251   5.6468  -6.5724\n",
       "  4.0019 -11.2075   8.3541  17.9453  -5.9804  -9.8933   4.7046  -8.9787\n",
       "  9.4583  -4.2251   4.6030   6.5112   0.4050  -3.8979  -3.7145  -6.4849\n",
       " -0.0343   8.4619   3.4680  -0.3063  -6.7246  -0.7935   0.4462  -5.2311\n",
       "  5.0599   1.7441  10.9733   3.8305  -0.5247  -1.1690  -1.8109  -5.6289\n",
       "  1.3023  -0.1380  -0.7845   6.4965  -1.3198   3.3718  -1.7696   0.7375\n",
       " -2.2294   5.9173  -4.8827 -10.2470  13.8279   5.3679   1.6391  13.5934\n",
       " -1.9410   7.9255  -7.2314 -11.6069   6.0629  18.5013  -5.2660  10.5784\n",
       "  2.9935   0.0056   2.9309   4.9164  -1.3866  -3.7487  12.8998  -2.0102\n",
       "  5.1775  -3.6782   2.7953   0.6018  -2.2346   1.7587  -4.7568  11.9662\n",
       " -6.3989   6.5406  -5.7232 -10.0707   3.9787   6.3956   0.1847   6.5440\n",
       "  1.5339  -3.8438   5.5306  11.2306  -1.8578  -5.6882   2.6861  -6.0967\n",
       "  0.3390  -4.0238   3.5321  15.9504   1.4054  -6.3943   6.9846 -12.0402\n",
       "  4.2120  -3.5445   1.6094   5.5750  -2.1975  -2.9206  -0.8673  -2.0676\n",
       "\n",
       "Columns 16 to 19 \n",
       " -0.2097  -5.7231   9.3414  -1.9698\n",
       "  2.3027  -1.8307  -2.6302   1.4957\n",
       " -0.4042   7.6887  -7.3123   0.5800\n",
       "  6.3166  -5.1372   7.6754  -5.4895\n",
       " -5.0011   0.1913   2.4757   1.7033\n",
       " -4.6323  12.7378  -6.8591  -0.7282\n",
       " -2.0911   0.8517   1.8592  -0.0158\n",
       "  3.8351 -10.4015  10.7471  -2.0428\n",
       " -0.1221  -3.5590  -0.1985  -1.4105\n",
       "  2.4739  -5.4145   0.1718  -0.9996\n",
       "  5.7189   4.7458  -0.6191  -3.7336\n",
       "  0.3037   2.1331  -0.1652   0.4053\n",
       " -0.4989   3.0117  -1.0997  -1.7079\n",
       " -2.0816   7.3101  -6.3696   3.7723\n",
       "  4.9800  -2.8487   3.5018  -2.0511\n",
       "  6.4252  -1.0513  -1.7707  -2.1042\n",
       "  4.0401   8.5751  -4.0344   4.3931\n",
       "  2.8509  12.2046   4.5162  -0.7003\n",
       " -1.2590   0.4961  11.8348  -2.0402\n",
       "  3.2290  -5.5068   3.7357  10.2837\n",
       "[torch.FloatTensor of size 20x20]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = Tensor(20,20).normal_()\n",
    "I=torch.inverse(M)\n",
    "D=torch.diag(torch.arange(1,21))\n",
    "Final= torch.mm(torch.mm(I,D),M)\n",
    "Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 7 \n",
       " 10.4045  -0.9419   3.6222  -1.4116  -9.6323  -0.4722  -7.3362  -4.9392\n",
       "  2.4257  13.2107   0.3904  -0.6006 -12.9330   4.9686  -6.4799 -12.9543\n",
       " -0.4191   3.1832  12.3707  -5.9989   5.4287  -2.7303   4.7215  -2.1032\n",
       "  5.2756   5.4096  -6.4435  11.5681  -4.8963  -4.9005  -2.3242  -1.8622\n",
       " -1.5717  -2.7760  -0.2458   3.3474  10.9153  -4.1733  -2.6637   4.1644\n",
       " -4.8710   2.4172   4.5613  -9.4318 -17.7283   9.9015 -13.0554 -17.4723\n",
       "  4.3985   6.4574  -2.4611   3.5491  18.5715  -3.9565  24.6780   6.3692\n",
       "  5.9939  -8.1849  -5.6538  13.1993  27.6506  -7.0502  19.3848  35.6359\n",
       " -3.2030  -0.0580   8.3566  -3.5954   8.5229   4.3063   7.5039   5.3046\n",
       " -1.0154  -0.1224   4.2139  -5.2779 -11.4177   2.8126  -7.3873  -9.7242\n",
       " -7.0897   9.3260  14.0330 -20.3737 -36.8542  11.1240 -28.6522 -40.9756\n",
       "  1.0487  -0.3103   0.8423   4.7398  13.3592   3.1186   9.8337  14.1720\n",
       "  4.6673   0.8005   3.9717  -0.0181 -17.2696   2.0292 -13.3538  -7.2577\n",
       "  2.0208  -3.5993  -3.3918   3.7153  -5.0572   0.5111  -3.4118  -2.1097\n",
       " -4.0065   7.1286   6.5069 -10.3063 -14.1470   3.8913 -11.6592 -20.1939\n",
       " -5.0193   6.4291  10.0716 -13.8396 -25.4169  10.3477 -17.9600 -23.6887\n",
       "  6.3087  -6.9438  -7.4668  19.4248  25.2931  -4.6381  17.5303  25.3612\n",
       " -4.3738   0.4374   5.7318  -1.2499  -5.3638   7.7193  -4.4429  -4.8367\n",
       " -4.4226  -2.7643   3.3692   1.9810   8.5245   2.6844   4.6132   5.6331\n",
       "  4.0511   2.5582  -1.0147   1.3630   1.2536  -4.4416  -0.8328  -2.1616\n",
       "\n",
       "Columns 8 to 15 \n",
       "  1.5312  -6.3111  -4.6383  -1.2435   5.9822   2.2578   6.1068   0.6711\n",
       "  3.5208 -10.9459  -6.5612   4.5694   6.5392   1.5386   9.0989  -4.1090\n",
       " -1.0289   1.3480  -1.7254  -0.6878   0.6981  -0.8108  -5.8001   3.7305\n",
       " -1.2600  -2.7823  -1.8136  -1.2868  -3.7830  -0.8276   1.8866   0.2143\n",
       " -1.3212   1.1410   1.7394  -4.5226  -3.6767   1.1362   3.0876   4.9520\n",
       "  3.2818 -16.8144 -15.7746  -1.4638  11.4085   7.7637  14.3382   2.2175\n",
       "  2.0399   7.1758   9.0876   5.7500  -7.4792  -1.7225 -15.2251  -6.4726\n",
       " -3.0911  23.5534  24.8866  -0.6749 -17.4700  -9.9569 -28.5706   2.8243\n",
       "  9.7571  -1.0900   1.0693   0.7225   4.4930   1.6089  -2.2487  -1.0906\n",
       " -0.5392   2.2686  -8.2341   1.2943   7.0837   2.9473   9.9054  -0.7800\n",
       " 10.0094 -36.3289 -29.2367   1.0888  29.5806  14.7310  39.0457  -2.1034\n",
       " -2.7288  11.5586  11.2754   9.1577  -7.4277  -6.1931  -9.8152   0.9371\n",
       "  3.7470 -10.1477  -8.7674  -1.2944  17.0326   1.3990  13.0552  -0.3055\n",
       "  0.8863  -1.8196   1.9757   2.0604  -0.6766  12.3668   2.2933  -3.1152\n",
       "  6.7638 -18.6197 -19.2862  -0.1233  14.0630   8.4214  29.8789  -1.1162\n",
       "  3.7666 -24.2072 -27.0261  -1.0167  18.4185   5.7338  22.8378   8.7723\n",
       " -3.1877  23.1987  28.4597   3.3419 -16.9571  -7.8414 -23.5862  -4.1149\n",
       "  4.7118  -6.9943  -4.2530   1.7448   9.8218   5.3384  11.8971  -3.1049\n",
       " -1.1981   6.2112   4.2136   0.6172  -0.6355  -1.8172  -5.3201   1.8259\n",
       "  2.8205  -3.0908   0.2613   0.4504  -1.1231   4.6326   5.2803  -2.2041\n",
       "\n",
       "Columns 16 to 19 \n",
       "  1.9133  -5.7679   7.5155   8.3720\n",
       "  1.3871 -12.4180  11.8458  11.6665\n",
       " -2.5164   2.8004  -0.1462  -0.5737\n",
       " -0.3136  -3.7242  12.4893   6.5853\n",
       "  2.3123   1.0905   3.6084  -0.6590\n",
       "  1.8910 -16.3371  20.6251  14.9243\n",
       " -6.1259  11.9989 -17.3142 -14.6204\n",
       " -6.5948  28.3124 -32.1930 -21.2723\n",
       " -1.9338   6.7541  -9.8138  -5.1296\n",
       "  2.9125  -9.8407   9.2051   9.1339\n",
       "  6.3636 -39.0347  41.3829  30.6878\n",
       " -2.1851  14.3034 -12.5244  -8.4172\n",
       "  2.2907  -9.1205  14.4952  12.8611\n",
       "  2.9185  -3.6444   2.1320   3.0372\n",
       "  1.4768 -17.7390  21.6715  13.5671\n",
       "  4.4512 -24.0184  32.4779  27.0237\n",
       "  9.7490  24.1192 -31.8104 -22.8541\n",
       "  2.9818   4.8533   3.6262   0.7940\n",
       " -0.1021   8.5131   0.1448  -7.0184\n",
       " -0.0868  -4.1307   0.5353   6.5710\n",
       "[torch.FloatTensor of size 20x20]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(123)\n",
    "m = Tensor(20, 20).normal_()\n",
    "d = torch.diag(torch.arange(1, m.size(0)+1))\n",
    "m.mm(d).mm(m.inverse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 7 \n",
       " 15.6072   3.4414   0.8763   3.7540   1.7988   5.4768  -3.4353  -5.8568\n",
       " -3.6168  15.1025  -8.4124  -2.6306  -7.0434  -6.0595  -0.3065 -19.8795\n",
       "  2.5168  -3.8492   9.1132   5.4769   3.1380   5.0749  -2.1999  14.1569\n",
       "  2.4748  -5.1602  -1.1142  14.7023   5.9249   2.8207  -3.9172  13.3794\n",
       "  1.7058  -0.9149  -4.0413  10.2710  11.8385   0.5849 -11.0590  -8.2239\n",
       "  0.2805  -3.8098   4.3133  -1.1580   3.8172  10.4567   2.2698  16.2155\n",
       "  4.3966  -3.4147   2.3066   6.5757   8.4903   6.5555   2.5430  15.2077\n",
       "  3.0979   0.1527   7.1539   0.3787   0.3746   3.8700  -4.8339   3.3772\n",
       "  4.1413  -4.1139  -0.8242   7.6256   4.1208   0.9881  -6.9881   2.2640\n",
       "  5.2799  -4.2572  -1.5540   5.8213   4.9300   6.4180  -1.7563  12.0843\n",
       " -1.7871  -0.3434  -2.1272  -6.4164  -0.1098  -4.4182   5.1287   3.4484\n",
       " -2.2182   0.8451  -5.1119  -1.8124   0.2871  -0.3469  -1.2646   0.0742\n",
       "  5.0578   1.1882   4.4971  -4.8879   5.9520   5.8881  -9.3567   3.6072\n",
       " -1.2728   7.3881  -1.9110  -3.7049  -7.1952  -2.9175   2.3439 -18.9995\n",
       " -1.9304  -2.2201  -2.1219   3.2338  -1.7106  -3.8631   0.9362  -1.0244\n",
       " -4.5521   1.7510  -3.0552  -8.8418  -8.6482 -11.2037   2.6430 -24.5206\n",
       " -3.8278   0.3350  -5.5668   1.6729  -1.1526  -0.0541   2.1294   4.4713\n",
       " -2.0960   0.2014  -2.2906  -1.2397   3.6773  -7.0938  -3.6238  -6.6630\n",
       " -1.7831   0.8206   0.1436  -0.6211  -1.1764   3.3518   8.8089  10.9628\n",
       "  3.3755  -4.1479   0.7011  10.6347   5.2449   5.8837  -5.0640  12.3477\n",
       "\n",
       "Columns 8 to 15 \n",
       " -2.0757  -4.4456  -2.1782  -3.1303   0.8563  -3.5052  -6.0820   3.7484\n",
       " -3.5212   1.1709 -18.6972  -3.5675  -0.0162  -8.2297  -9.7272  -0.4467\n",
       "  4.9421  -1.7791   8.5331  -0.1688   0.1451   6.0696   5.7195   0.2153\n",
       "  3.4304  -0.4017  14.9365  -1.2597   1.1297   8.6242   8.2816  -2.7986\n",
       "  2.6496  -5.5707  -6.9643  -6.4575   4.5088  -3.4791 -12.9193  -0.9358\n",
       "  0.1223   3.0744  15.7571   3.6732   0.1101   7.2202   9.9504  -3.9648\n",
       " -1.0628  -4.0083  17.8753  -5.0450   1.7918  10.2017   5.0084  -2.8875\n",
       "  1.1268  -3.3089  -4.9466   0.2100   0.1015  -0.4079  -7.0002  -0.5309\n",
       " 11.2196  -1.6451   7.0942  -4.6910   3.1408   2.8609   1.1707   0.1397\n",
       "  0.0268  10.1674  13.2346   2.7013  -2.2500   2.3024   5.8466   0.3960\n",
       " -3.7734   5.0517  15.7318   3.0611  -1.9468  -0.2271   6.5059  -3.2928\n",
       " -0.3214   0.3322  -2.8034  11.8547  -1.8413  -0.7279  -0.4970  -3.9835\n",
       "  2.3352  -5.2968   2.3717  -1.5816   8.6281  -1.4841   0.1945  -8.0280\n",
       " -4.1291  -0.2583 -20.7094   0.8742  -0.7438  -3.2034 -12.0726   4.1089\n",
       "  2.5875   1.2297   1.2139  -3.3775   3.6126   2.1144  12.3577   2.5362\n",
       "  1.2115   1.6000 -22.3822  -3.1240  -1.9817 -12.2730  -8.9167  11.6337\n",
       "  3.1421   0.1447  -1.6363  -0.8062  -2.1792   3.1243   4.4568   1.2617\n",
       "  0.0320  -2.1804   3.7383  -7.2801   5.2047  -3.5970  -1.1837  -2.7890\n",
       " -1.6788   2.1061   5.2398   3.6688  -1.5792   3.5818   4.7952   2.7236\n",
       "  4.9816  -1.8347  10.4672   1.9298   1.2355   9.4438   2.3752  -1.9629\n",
       "\n",
       "Columns 16 to 19 \n",
       " -2.5355  -1.3384  -1.5171   3.1888\n",
       "-11.5939  -1.4030  -4.4880   3.0214\n",
       "  9.3353   1.3036   3.6453  -1.3368\n",
       " 11.2655  -0.3399   3.7097  -1.4351\n",
       " -0.8960   1.7095   0.8008   1.8967\n",
       "  8.2409   0.4690   3.9378  -4.3750\n",
       " 13.3792   1.5423   6.8440  -1.8199\n",
       " -3.2316  -0.5980  -1.3286  -3.4878\n",
       "  6.0920  -0.5319   1.9992   0.5449\n",
       "  8.1313  -2.5663  -0.0036  -4.0127\n",
       " -1.9788  -1.0970  -2.7951  -2.0333\n",
       " -1.2822  -1.1385  -2.1424  -0.2305\n",
       "  2.0484  -1.1405  -2.2800  -8.8156\n",
       "-14.9681  -0.4594  -5.0667   5.4067\n",
       "  2.3414   1.0650   2.3855   3.4650\n",
       "-13.9471  -3.4420  -6.6695  -0.8304\n",
       " 17.1187   1.3700   3.0606   2.3027\n",
       " -0.4462   9.1748  -1.8644   4.6547\n",
       "  1.3718   0.2605  13.9527   1.3667\n",
       " 10.6947   2.6738   3.6354   8.6238\n",
       "[torch.FloatTensor of size 20x20]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = Tensor(20,20).normal_()\n",
    "I=torch.inverse(M)\n",
    "D=torch.diag(torch.arange(1,21))\n",
    "Final= torch.mm(M,torch.mm(D,I))\n",
    "Final\n",
    "eigenV, _ = Final.eig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Flops per second \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput 8.628515e+10 flop/s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "d=5000\n",
    "m = Tensor(d, d).normal_()\n",
    "n = Tensor(d, d).normal_()\n",
    "time1 = time.perf_counter()\n",
    "torch.mm(m,n)\n",
    "time2 = time.perf_counter()\n",
    "print('Throughput {:e} flop/s'.format((d * d * d)/(time2 - time1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Playing with strides "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed ratio 388.0750513810721\n",
      "Sanity check: error is  0.0\n"
     ]
    }
   ],
   "source": [
    "def mul_row(m):\n",
    "    temp = torch.Tensor(m.size())\n",
    "    for i in range(0,m.size(0)):\n",
    "        for j in range(0,m.size(1)):\n",
    "            temp[i,j]= m[i,j]*(i+1)\n",
    "    return temp\n",
    "    \n",
    "    \n",
    "def mul_row_fast(m):\n",
    "    d = m.size(0)\n",
    "    temp = torch.arange(1, d + 1).view(d, 1).expand_as(m) #first we create a vector with the same lenght as there are rows in m, \n",
    "                                                        #then we reshape it and then we expend it to be of the same size than m\n",
    "\n",
    "    return m.mul(temp)\n",
    "\n",
    "m = Tensor(10000, 400).normal_(5.0)\n",
    "\n",
    "time1 = time.perf_counter()\n",
    "a = mul_row(m)\n",
    "time2 = time.perf_counter()\n",
    "b = mul_row_fast(m)\n",
    "time3 = time.perf_counter()\n",
    "\n",
    "print('Speed ratio', (time2 - time1) / (time3 - time2))\n",
    "\n",
    "print('Sanity check: error is ', torch.norm(a - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
